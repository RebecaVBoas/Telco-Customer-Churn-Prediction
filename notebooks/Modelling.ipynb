{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d675c01-952b-4af7-b498-fdc5b24df1c7",
   "metadata": {},
   "source": [
    "# Telco Custumer Churn\n",
    "\n",
    "Neste projeto irei construir um modelo de machine learnig para prever a possibilidade de um cliente cancelar o contrato com uma operadora de telecomunicações. Trata-se de um problema de aprendizado supervisionado (utilizando um conjunto de dados rotulado) de classificação, em que o alvo (target) é 1 se o cliente cancelou o serviço e 0 caso contrário.\n",
    "\n",
    "Pipiline de resolução usado no projeto(baseado no CRISPIM-DM)\n",
    "1. Definir o problema de negócio.\n",
    "\n",
    "2. Coletar os dados e obter uma visão geral deles.\n",
    "\n",
    "3. Dividir os dados em conjuntos de treino e teste.\n",
    "\n",
    "4. Explorar os dados (Análise Exploratória de Dados – EDA).\n",
    "\n",
    "5. Engenharia de atributos, limpeza e pré-processamento dos dados.\n",
    "\n",
    "6. Treinamento dos modelos, comparação, seleção de atributos e ajuste de hiperparâmetros.\n",
    "\n",
    "7. Teste final do modelo em produção e avaliação.\n",
    "\n",
    "8. Conclusão e interpretação dos resultados do modelo.\n",
    "\n",
    "9. Implantação (deploy).\n",
    "\n",
    "\n",
    "Neste notebook, irei realizar a modelagem de machine learning, cobrindo as etapas 5 a 8 do pipeline apresentado acima. O principal objetivo aqui é construir um modelo que possa prever com precisão a probabilidade de um cliente cancelar (churn). Após construir esse modelo, a empresa de telecomunicações podera prever a probabilidade de um cliente sair, e direcionar suas estratégias para retenção e fidelização do cliente. Adquirir novos clientes é mais caro do que manter os existentes. Além disso, abordarei essas etapas em detalhes a seguir, explicando o motivo de cada decisão tomada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847dcfa1-1997-45e3-84dd-5cceb015c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização e manipulação do dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8a8cb1-c7ad-4672-95ec-ee88c5248c90",
   "metadata": {},
   "source": [
    "## Carregando os dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bbaf90-944c-4e4d-8aca-e1bf991ac502",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_patch = '../data/WA_Fn-UseC_-Telco-Customer-Churn.xls'\n",
    "\n",
    "df = pd.read_csv(data_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d37a6-48e5-46d1-8fd7-ef9819a40d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2fd74c-0e58-4a1f-a8a5-d09275dd8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e90515-f582-44d7-aa4a-3207575f294c",
   "metadata": {},
   "source": [
    "Irei transformar a coluna \"TotalCharges\" para o tipo float64, pois o mesmo é composta por dados númericos e substituir os valores nulos de \"TotalCharges\" por zero, pois os mesmo são referentes aos clientes que ainda não completaram um mês de serviço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49202b8-a30d-458c-a6a5-472cc0396c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b072a6-e5b1-4d07-a1b6-9e5ef8dbd964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['tenure'] == 0, 'TotalCharges'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79daf7-68a9-4a1f-9853-6117f4ab723e",
   "metadata": {},
   "source": [
    "PROXIMOS PASSOS\n",
    "1. Carregamento e Tipagem (Antes do Split)\n",
    "mudar as variaveis objetos para cateory e mapear os valores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb5b04e-0831-4e8e-907c-97fbfc1d37e6",
   "metadata": {},
   "source": [
    "Irei remover algumas tabelas, cujos valores não contribuem de forma significativa para a análise, e padronizar o nome das colunas (tenure e gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6a130-6dc7-491f-93e4-29f069fdfc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['customerID', 'gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef13c5-d5fa-403b-98a1-2d4e7f0a164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padronizando nomes das colunas\n",
    "df = df.rename(columns={ 'tenure' : 'Tenure' })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f20fb-5bab-44cd-b2ab-84c8cad38b65",
   "metadata": {},
   "source": [
    "## 3. Separando os dados entre treino e test (Split)\n",
    "\n",
    "* Irei separar os dados entre treino e teste para evitar o DataLekage, e garantir a capacidade de genereralização do modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fcbe74-bb1f-4939-b956-00273761ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = ['Churn'])\n",
    "y = df['Churn'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state =42)\n",
    "\n",
    "print(f\"Treino: {X_train.shape[0]} amostras | Teste: {X_test.shape[0]} amostras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f9c485-acc8-4ab2-95e0-c734e28a9837",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Treno: {y_train.value_counts(normalize = True)}\")\n",
    "print(f\"Teste: {y_test.value_counts(normalize = True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952bdf46-32c4-4cac-9d78-17e929961d3e",
   "metadata": {},
   "source": [
    "* as proporções entre treino e teste foram mantidas.\n",
    "\n",
    "## 5. Engenharia de atributos, limpeza e pré-processamento dos dados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e42ebd-e0f8-4c9d-912b-df2163cfd59a",
   "metadata": {},
   "source": [
    "Nesta próxima etapa, transformarei os dados brutos em sinais claros e acionáveis para os algoritmos. O foco não será apenas a \"limpeza\", mas sim garantir que o modelo final esteja protegido contra viés, vazamento de dados (*data leakage*) e instabilidade numérica.\n",
    "\n",
    "Abaixo, detalho as decisões estratégicas que implementarei na preparação do dataset:\n",
    "\n",
    "\n",
    "#### 1. Engenharia de Atributos (Feature Engineering)\n",
    "\n",
    "Embora eu remova a variável TotalCharges original para evitar multicolinearidade com tenure, criarei novas métricas para capturar padrões:\n",
    "\n",
    "* **Serviços De Segurança:** Criarei uma contagem de quantos serviços extras o cliente possui ('TechSupport', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection').Vimos que os clientes que utilizam esses serviços tem baixa aderencia de churn. Usaremos essa informação para identificar os clientes que não utilizam esse serviço, e que consequentemente tem mais propensão ao churn.\n",
    "  \n",
    "* **Segmentação de Faturamento:** Realizarei a binatização de faturas muito acima da média para identificar explicitamente o \"choque de preço\" notado na EDA.\n",
    "\n",
    "#### 2. Codificação de Variáveis Categóricas (Encoding)\n",
    "\n",
    "Como os modelos matemáticos não interpretam texto, traduzirei as categorias em números, tomando cuidado para não criar hierarquias falsas:\n",
    "\n",
    "* **One-Hot Encoding (OHE):** Aplicarei em variáveis como PaymentMethod. Para evitar a \"Armadilha da Variável Dummy\", utilizarei o parâmetro drop='first, removendo colunas redundantes.\n",
    "* **Ordinal/Binary Encoding:** Para variáveis \"Yes/No\", utilizarei uma codificação binária simples (0 e 1), mantendo o dataset leve.\n",
    "\n",
    "#### 3. Escalonamento de Atributos (Feature Scaling)\n",
    "\n",
    "Para garantir o bom desempenho de algoritmos sensíveis à escala (baseados em distância ou gradiente), padronizarei os dados numéricos.\n",
    "\n",
    "* **StandardScaler:** Aplicarei a padronização () nas variáveis tenure e MonthlyCharges.\n",
    "* **Prevenção de Data Leakage:** Garantirei que o *fit* do escalonador seja feito **exclusivamente no conjunto de treino**. O conjunto de teste será apenas transformado, simulando dados inéditos em produção.\n",
    "\n",
    "#### 4. Arquitetura de Pipeline\n",
    "\n",
    "Para tornar todo esse processo reprodutível e robusto, utilizarei o ColumnTransformer dentro de um Pipeline do Scikit-Learn. Isso encapsulará o pré-processamento e o modelo em um único objeto treinável.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643aa55-f74a-47fb-bdce-163d2653b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([X_train, y_train], axis = 1)\n",
    "\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f9440d-4fd3-465c-9232-61540f25140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([X_test, y_test], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92412440-0250-4485-8445-4b9d58630962",
   "metadata": {},
   "source": [
    "* Irei criar a contagem de servições de segurança o cliente possui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b18336-2cbf-478a-af5b-a60e5b46d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retention_services = ['TechSupport', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection']\n",
    "\n",
    "df_train['Security_Svc_Count'] = (df_train[retention_services] == 'Yes').sum(axis=1)\n",
    "\n",
    "df_train.drop(columns=retention_services, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404c202-7cc2-4c04-82d2-b96309e2ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementar as mudanças no teste \n",
    "retention_services = ['TechSupport', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection']\n",
    "\n",
    "df_test['Security_Svc_Count'] = (df_test[retention_services] == 'Yes').sum(axis=1)\n",
    "\n",
    "df_test.drop(columns=retention_services, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4db5115-1db7-4112-b766-1ccc8f19fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "* Irei criar a fragmentação do faturamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc4965f-2b33-4480-90ea-cd466778fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "limiar_treino = df_train['MonthlyCharges'].quantile(0.80)\n",
    "\n",
    "def aplicar_choque_faturamento(df, corte):\n",
    "    df['Is_Price_Shock'] = (df['MonthlyCharges'] > corte).astype(int)\n",
    "    return df\n",
    "\n",
    "df_train = aplicar_choque_faturamento(df_train, limiar_treino)\n",
    "\n",
    "#utilizei o valor calculado no teste, no treino para evitar o vazamento de dados \n",
    "df_test = aplicar_choque_faturamento(df_test, limiar_treino) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e80bcf5-45f1-4c44-8ef2-bb3b3d56ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_features = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling','Churn',\n",
    "    'MultipleLines', 'StreamingTV', 'StreamingMovies', 'SeniorCitizen']\n",
    "\n",
    "mapping = {'Yes': 1, 'No': 0, 'No internet service': 0, 'No phone service': 0}\n",
    "\n",
    "for col in binary_features:\n",
    "    if col in df_train.columns:\n",
    "        df_train[col] = df_train[col].map(mapping).fillna(0).infer_objects(copy=False).astype(int)\n",
    "        \n",
    "    if col in df_test.columns:\n",
    "        df_test[col] = df_test[col].map(mapping).fillna(0).infer_objects(copy=False).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c8f08f-2445-4199-9b16-5ee83080398c",
   "metadata": {},
   "source": [
    "Para a codificação das variáveis categóricas, optei pelo uso do objeto OneHotEncoder do Scikit-Learn em vez do método pd.get_dummies. Esta escolha fundamenta-se em três pilares estratégicos:\n",
    "\n",
    "* Prevenção de Data Leakage. Diferente do get_dummies, que atua de forma global, o OneHotEncoder permite isolar o aprendizado do vocabulário estritamente no conjunto de treino. Esse conhecimento é então aplicado ao conjunto de teste, garantindo uma avaliação imparcial e realista do modelo.\n",
    "\n",
    "* O uso do encoder garante que a estrutura de colunas (o shape do dataset) seja idêntica em ambos os conjuntos. Com o parâmetro handle_unknown='ignore', o pipeline torna-se resiliente a novas categorias presentes apenas nos dados de teste ou produção, evitando falhas de execução no modelo.\n",
    "\n",
    "* Ao reconstruir os DataFrames pós-transformação respeitando os índices originais, asseguro a perfeita integridade dos dados durante a concatenação, eliminando o risco de desalinhamento de informações sensíveis dos clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff841c-9940-4675-852e-4e6d776fffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_features = ['Contract', 'PaymentMethod', 'InternetService']\n",
    "\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "encoded_train = encoder.fit_transform(df_train[one_hot_features])\n",
    "\n",
    "encoded_test = encoder.transform(df_test[one_hot_features])\n",
    "\n",
    "encoded_cols = encoder.get_feature_names_out(one_hot_features)\n",
    "\n",
    "df_train_encoded = pd.DataFrame(encoded_train, columns=encoded_cols, index=df_train.index)\n",
    "df_test_encoded = pd.DataFrame(encoded_test, columns=encoded_cols, index=df_test.index)\n",
    "\n",
    "df_train = pd.concat([df_train.drop(one_hot_features, axis=1), df_train_encoded], axis=1)\n",
    "df_test = pd.concat([df_test.drop(one_hot_features, axis=1), df_test_encoded], axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
